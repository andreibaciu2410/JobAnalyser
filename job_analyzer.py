import streamlit as st
import os
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Optional, Literal
from pydantic import BaseModel, Field, model_validator
import instructor
from groq import Groq
from dotenv import load_dotenv

# ==============================================================================
# 1. SETUP & SECURITATE
# ==============================================================================
st.set_page_config(page_title="GenAI Headhunter", page_icon="üïµÔ∏è", layout="wide")

# √éncƒÉrcƒÉm variabilele din fi»ôierul .env
load_dotenv()

# √éncercƒÉm sƒÉ luƒÉm cheia din OS (local) sau din Streamlit Secrets (cloud)
api_key = os.getenv("GROQ_API_KEY")

# Fallback pentru Streamlit Cloud deployment
if not api_key and "GROQ_API_KEY" in st.secrets:
    api_key = st.secrets["GROQ_API_KEY"]

# Validare criticƒÉ: DacƒÉ nu avem cheie, oprim aplica»õia aici.
if not api_key:
    st.error("‚õî EROARE CRITICƒÇ: Lipse»ôte `GROQ_API_KEY`.")
    st.info("Te rog creeazƒÉ un fi»ôier `.env` √Æn folderul proiectului »ôi adaugƒÉ: GROQ_API_KEY=cheia_ta_aici")
    st.stop()

# Configurare Client Groq Global (pentru a nu-l reini»õializa constant)
client = instructor.from_groq(Groq(api_key=api_key), mode=instructor.Mode.TOOLS)

# Sidebar Informativ (FƒÉrƒÉ input de date sensibile)
with st.sidebar:
    st.header("üïµÔ∏è GenAI Headhunter")
    st.success("‚úÖ API Key √ÆncƒÉrcat securizat")
    st.markdown("---")
    st.write("Acest tool demonstreazƒÉ:")
    st.write("‚Ä¢ Web Scraping (BS4)")
    st.write("‚Ä¢ Secure Env Variables")
    st.write("‚Ä¢ Structured Data (Pydantic)")


# ==============================================================================
# 2. DATA MODELS (PYDANTIC SCHEMAS)
# ==============================================================================

class SalaryRange(BaseModel):
    min: int = Field(..., description="Salariul minim")
    max: int = Field(..., description="Salariul maxim")
    currency: str = Field(..., description="Moneda de plata")

class Location(BaseModel):
    city: Optional[str] = Field(..., description="Locatia jobului")
    country: Optional[str] = Field(..., description="Tara jobului")
    is_remote: bool = Field(False, description="Jobul este remote sau nu")

class RedFlag(BaseModel):
    severity: Literal["low", "medium", "high"] = Field(..., description="Severitatea red flag-ului")
    category: Literal["toxicity", "vague", "unrealistic"] = Field(..., description="Categoria red flag-ului")
    message: str = Field(..., description="Mesajul red flag-ului")

class RawExtraction(BaseModel):
    role_title: Optional[str] = None
    company_name: Optional[str] = None
    tech_stack: List[str] = Field(default_factory=list)

    salary_range: Optional[SalaryRange] = None
    location: Optional[Location] = None
    is_remote: Optional[bool] = None  # ce reiese explicit din text

    requirements: List[str] = Field(..., description="Cerinte explicite, bullet-like")
    responsibilities: List[str] = Field(..., description="Responsabilitati explicite")
    benefits: List[str] = Field(..., description="Beneficii explicite")
    red_flags: List[RedFlag] = Field(..., description="Doar daca sunt sugerate clar de text")

    @model_validator(mode="after")
    def normalize_location(self):
        if self.location and not self.location.city and not self.location.country:
            self.location = None
        return self
    
class JobAnalysis(BaseModel):
    role_title: str = Field(..., description="Titlul jobului standardizat")
    company_name: str = Field(..., description="Numele companiei")
    seniority: Literal["Intern", "Junior", "Mid", "Senior", "Lead", "Architect", "Unknown"] = Field("Unknown", description="Nivelul de experien»õƒÉ dedus")
    match_score: int = Field(..., ge=0, le=100, description="Scor 0-100: Calitatea descrierii jobului")
    tech_stack: List[str] = Field(..., description="ListƒÉ cu tehnologii specifice (ex: Python, AWS, React)")
    red_flags: List[RedFlag] = Field(default_factory=list, description="Lista de semnale de alarmƒÉ (toxicitate, stres, vaguitate)")
    summary: str = Field(..., description="Un rezumat scurt al rolului (max 2 fraze) √Æn limba rom√¢nƒÉ")
    is_remote: bool = Field(False, description="True dacƒÉ jobul este remote sau hibrid")
    salary_range: SalaryRange = Field(..., description="Range salarial")
    location: Location = Field(..., description="Detaliile locatiei jobului")

    @model_validator(mode="after")
    def remote_location_consistency(self):
        # sa avem consistenta intre cele 2 campuri
        if self.is_remote != self.location.is_remote:
            self.red_flags.append(
                RedFlag(
                    severity="medium",
                    category="vague",
                    message=f"Inconsistenta intre is_remote={self.is_remote} si location.is_remote={self.location.is_remote}"
                )
            )
        
        if self.is_remote:
            office_patterns = re.compile(r"\b(office|on[-\s]?site|onsite|hybrid|hibrid|birou|sediu|headquarters|hq)\b", re.IGNORECASE)
            combined = f"{self.location.city} {self.location.country}"

            if office_patterns.search(combined):
                self.red_flags.append(
                    RedFlag(
                        severity="medium",
                        category="vague",
                        message="Remote=True, dar c√¢mpurile de loca»õie con»õin indicii de prezen»õƒÉ la birou"
                    )
                )

        return self

class StrategicAdvice(BaseModel):
    fit_summary: str = Field(..., description="2-4 fraze: cum suna rolul si pentru cine e potrivit")
    interview_questions: List[str] = Field(..., description="Intrebari concrete pentru clarificari")
    negotiation_angles: List[str] = Field(..., description="Argumente / tactici pentru negociere")
    risk_notes: List[str] = Field(..., description="Riscuri si cum le verifici")
    next_steps: List[str] = Field(..., description="Pasi urmatori")

class ValidationIssue(BaseModel):
    field: str = Field(..., description="Calea campului din RawExtraction/JobAnalysis care are problema")
    severity: Literal["low", "medium", "high"] = Field(..., description="Cat de grava e inconsistenta")
    message: str = Field(..., description="Explicatie scurta despre de ce e problema si ce impact are")
    evidence: Optional[str] = Field(None, description="Fragment scurt din text care sustine sau contrazice")

class ValidationReport(BaseModel):
    cleaned: RawExtraction
    issues: List[ValidationIssue] = Field(default_factory=list)
    confidence: int = Field(100, ge=0, le=100, description="Cat de bine se pot verifica faptele din text")


# ==============================================================================
# 3. UTILS - SCRAPER (Colectare Date)
# ==============================================================================

def scrape_clean_job_text(url: str, max_chars: int = 3000) -> str:
    """
    DescarcƒÉ pagina »ôi returneazƒÉ un text curat, optimizat pentru contextul LLM.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return f"Error: Status code {response.status_code}"
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # EliminƒÉm elementele inutile care consumƒÉ tokeni
        for junk in soup(["script", "style", "nav", "footer", "header", "aside", "iframe"]):
            junk.decompose()
            
        # Extragem textul »ôi eliminƒÉm spa»õiile multiple
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text)
        
        return text[:max_chars] 
        
    except Exception as e:
        return f"Scraping Error: {str(e)}"

# ==============================================================================
# 4. AI SERVICE LAYER (Logica LLM)
# ==============================================================================

def analyze_job_with_ai(text: str) -> JobAnalysis:
    """
    Trimite textul curƒÉ»õat cƒÉtre Groq »ôi returneazƒÉ obiectul structurat.
    """
    return client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        response_model=JobAnalysis,
        messages=[
            {
                "role": "system", 
                "content": (
                    "E»ôti un Recruiter Expert √Æn IT. AnalizeazƒÉ textul jobului cu obiectivitate. "
                    "IdentificƒÉ tehnologiile »ôi poten»õialele probleme (red flags). "
                    "RƒÉspunde strict √Æn formatul cerut."
                )
            },
            {
                "role": "user", 
                "content": f"AnalizeazƒÉ acest job description:\n\n{text}"
            }
        ],
        temperature=0.1,
    )

# ==============================================================================
# 5. Agent 1: determinist, doar fapte
# ==============================================================================

def extract_job_facts(text: str) -> RawExtraction:
    return client.chat.completions.create(
        model="qwen/qwen3-32b",
        response_model=RawExtraction,
        messages=[
            {
                "role": "system",
                "content": (
                    "E»ôti The Extractor. Extragi DOAR fapte brute din textul jobului. "
                    "Nu inventa. DacƒÉ lipse»ôte informa»õia, lasƒÉ null / listƒÉ goalƒÉ. "
                    "Nu oferi sfaturi. Nu rezuma."
                ),
            },
            {"role": "user", "content": f"Extrage faptele din acest job:\n\n{text}"},
        ],
        temperature=0.0,
    )

# ==============================================================================
# 6. Agent 2: creativ, insight + strategie
# ==============================================================================

def generate_counceling(facts: RawExtraction) -> StrategicAdvice:
    return client.chat.completions.create(
        model="qwen/qwen3-32b",
        response_model=StrategicAdvice,
        messages=[
            {
                "role": "system",
                "content": (
                    "E»ôti The Counselor. Prime»ôti fapte structurate despre un job »ôi oferi "
                    "insight-uri strategice: potrivire, √ÆntrebƒÉri de interviu, negociere salariu, riscuri."
                ),
            },
            {
                "role": "user",
                "content": (
                    "GenereazƒÉ advice strategic pe baza acestor fapte (JSON):\n\n"
                    f"{facts.model_dump_json(indent=2, exclude_none=True)}"
                ),
            },
        ],
        temperature=0.7,
    )

# ==============================================================================
# 7. Agent 3: The Validator
# ==============================================================================

def validate_extraction(original_text: str, facts: RawExtraction) -> ValidationReport:
    return client.chat.completions.create(
        model="llama-3.1-8b-instant",
        response_model=ValidationReport,
        messages=[
            {
                "role": "system",
                "content": (
                    "E»ôti The Validator. Verifici consisten»õa dintre textul jobului »ôi JSON-ul RawExtraction. "
                    "Nu inventa dovezi. DacƒÉ un c√¢mp nu e sus»õinut explicit, pune-l la null/empty √Æn cleaned "
                    "»ôi adaugƒÉ un issue. DacƒÉ e contrazis, severity=high."
                    "\nReguli:\n"
                    "- tech_stack: doar tehnologii prezente explicit\n"
                    "- salary_range: doar dacƒÉ existƒÉ cifre/monedƒÉ √Æn text\n"
                    "- location/is_remote: trebuie sƒÉ fie sus»õinute de text\n"
                    "- requirements/benefits: trebuie sƒÉ fie parafraze scurte din text\n"
                    "ReturneazƒÉ cleaned + issues + confidence."
                ),
            },
            {
                "role": "user",
                "content": (
                    "TEXT JOB:\n"
                    f"{original_text}\n\n"
                    "RAW EXTRACTION (JSON):\n"
                    f"{facts.model_dump_json(indent=2, exclude_none=True)}"
                ),
            },
        ],
        temperature=0.0,
    )

# ==============================================================================
# 7. UI - APLICA»öIA STREAMLIT
# ==============================================================================

st.title("üïµÔ∏è GenAI Headhunter Assistant")
st.markdown("TransformƒÉ orice Job Description √Æntr-o analizƒÉ structuratƒÉ folosind AI.")

# Tab-uri
tab1, tab2 = st.tabs(["üöÄ AnalizƒÉ Job", "üìä Market Scan (Batch)"])

# --- TAB 1: ANALIZA UNUI SINGUR LINK ---
with tab1:
    st.subheader("AnalizeazƒÉ un Job URL")
    url_input = st.text_input("Introdu URL-ul:", placeholder="https://...")
    
    if st.button("AnalizeazƒÉ Job", key="btn_single"):
        if not url_input:
            st.warning("Te rugƒÉm introdu un URL.")
        else:
            with st.spinner("üï∑Ô∏è Scraping & ü§ñ AI Analysis..."):
                raw_text = scrape_clean_job_text(url_input)
            
            if "Error" in raw_text:
                st.error(raw_text)
            else:
                try:
                    data = analyze_job_with_ai(raw_text)
                    report = extract_job_facts(raw_text)
                    facts = validate_extraction(raw_text, report) 
                    advice = generate_counceling(facts.cleaned)
                    
                    #analyze_job_with_ai
                    # -- DISPLAY --
                    st.divider()
                    col_h1, col_h2 = st.columns([3, 1])
                    with col_h1:
                        st.markdown(f"### {data.role_title}")
                        st.caption(f"Companie: **{data.company_name}** | Nivel: **{data.seniority}**")
                        st.caption(f"Locatie: **{data.location.country}**, **{data.location.city}**")
                        st.caption(f"Remote: **{'Da' if data.location.is_remote else 'Nu'}**")
                        st.caption(f"Salariu: **{data.salary_range.min}** - **{data.salary_range.max}** **{data.salary_range.currency}**")
                    with col_h2:
                        color = "normal" if data.match_score > 70 else "inverse"
                        st.metric("Quality Score", f"{data.match_score}/100", delta_color=color)

                    # Detalii
                    c1, c2, c3 = st.columns(3)
                    c1.info(f"**Remote:** {'Da' if data.is_remote else 'Nu'}")
                    c2.success(f"**Tehnologii:** {len(data.tech_stack)}")
                    c3.error(f"**Red Flags:** {len(data.red_flags)}")

                    st.markdown(f"**üìù Rezumat:** {data.summary}")
                    st.markdown("#### üõ†Ô∏è Tech Stack")
                    st.write(", ".join([f"`{tech}`" for tech in data.tech_stack]))

                    if data.red_flags:
                        st.markdown("#### üö© Avertismente")
                        for flag in data.red_flags:
                            st.warning(f"‚ö†Ô∏è {flag}")

                    st.divider()
                    # extract_job_facts
                    #generate_strategic_advice
                    st.markdown("### üßæ Facts (Extractor)")
                    st.json(facts.model_dump(exclude_none=True))

                    st.markdown("### üß† Advice (Counselor)")
                    st.write(advice.fit_summary)

                    with st.expander("üé§ Interview Questions"):
                        for q in advice.interview_questions:
                            st.write(f"- {q}")

                    with st.expander("üí∞ Negotiation Angles"):
                        for a in advice.negotiation_angles:
                            st.write(f"- {a}")

                    with st.expander("‚ö†Ô∏è Risks & Checks"):
                        for r in advice.risk_notes:
                            st.write(f"- {r}")

                    with st.expander("‚úÖ Next Steps"):
                        for n in advice.next_steps:
                            st.write(f"- {n}")

                except Exception as e:
                    st.error(f"Eroare AI: {str(e)}")

# --- TAB 2: BATCH PROCESSING ---
with tab2:
    st.subheader("üìä ComparƒÉ mai multe joburi")
    urls_text = st.text_area("Paste URL-uri (unul pe linie):", height=150)
    
    if st.button("ScaneazƒÉ Pia»õa", key="btn_batch"):
        urls = [u.strip() for u in urls_text.split('\n') if u.strip()]
        
        if not urls:
            st.warning("Nu ai introdus link-uri.")
        else:
            results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, link in enumerate(urls):
                status_text.text(f"Analizez {i+1}/{len(urls)}...")
                text = scrape_clean_job_text(link)
                
                if "Error" not in text:
                    try:
                        res = analyze_job_with_ai(text)
                        results.append({
                            "Role": res.role_title,
                            "Company": res.company_name,
                            "Seniority": res.seniority,
                            "Tech": res.tech_stack,
                            "Score": res.match_score
                        })
                    except:
                        pass # ContinuƒÉm chiar dacƒÉ unul crapƒÉ
                
                progress_bar.progress((i + 1) / len(urls))
            
            status_text.text("Gata!")
            
            if results:
                df = pd.DataFrame(results)
                st.dataframe(df)
                
                # Grafic simplu
                st.bar_chart(df['Seniority'].value_counts())